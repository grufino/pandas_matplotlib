{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark\n",
    "\n",
    "Apache Spark is an open-source distributed general-purpose cluster-computing framework. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n",
    "\n",
    "During this lecture we try out some basic spark concepts and operations. We start by loading a data into a Resilient Distributed Data Dataset (RDD) and perform some basic transformations and actions on these data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDD from Python list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with Spark at Databricks, a spark context (`SparkContext`), which represents the connection to a Spark cluster, and is used for create RDDs, is automatically created for you, when you attach a notebook to a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data, 4)\n",
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Spark Tranformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "\n",
    "`map(func)` return a new distributed dataset formed by passing each element of the source through a function *func*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rdd = rdd.map(lambda x: x * 2)\n",
    "result_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter\n",
    "\n",
    "`filter(func)` return a new dataset formed by selecting those elements of the source on which *func* returns `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_result = rdd.filter(lambda x: x % 2 == 0) \n",
    "rdd_result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distinct\n",
    "\n",
    "`distinct(func)` return a new dataset that contains the distinct elements of the source dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = sc.parallelize([1, 4, 2, 2, 3]) \n",
    "result_rdd = rdd2.distinct()\n",
    "result_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flat Map\n",
    "\n",
    "`flatMap(func)` similar to `map`, but each input item can be mapped to 0 or more output items (so *func* should return a *Seq* rather than a single item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3])\n",
    "rdd_result = rdd.map(lambda x: [x, x+5])\n",
    "rdd_result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_result = rdd.flatMap(lambda x: [x, x+5]) \n",
    "rdd_result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Spark Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce\n",
    "\n",
    "`reduce(func)` aggregate dataset’s elements using function *func*. *func* takes two arguments and returns one, and is commutative and associative so that it can be computed correctly in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3]) \n",
    "rdd.reduce(lambda a, b: a * b) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take\n",
    "\n",
    "`take(n)` return an array with the first *n* elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect\n",
    "\n",
    "`collect()` return all the elements as an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take Ordered\n",
    "\n",
    "`takeOrdered(n, key=func)` return *n* elements ordered in ascending order or as specified by the optional key function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([5,3,1,2])\n",
    "rdd.takeOrdered(3, lambda s: -1 * s) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Key-Value Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce By Key\n",
    "\n",
    "`reduceByKey(func)` return a new distributed dataset of (K,V) pairs where the values for each key are aggregated using the given reduce function *func*, which must be of type (V,V)➞V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,2), (3,4), (3,6)]) \n",
    "rdd.reduceByKey(lambda a, b: a + b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort By Key\n",
    "\n",
    "`sortByKey(func)` return a new dataset (K,V) pairs sorted by keys in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = sc.parallelize([(1,'a'), (2,'c'), (1,'b')]) \n",
    "rdd2.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By Key\n",
    "\n",
    "`groupByKey(func)` return a new dataset of `(K, Iterable<V>)` pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = sc.parallelize([(1,'a'), (2,'c'), (1,'b')]) \n",
    "rdd2.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output resulting iterables as lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2.groupByKey().map(lambda x: (x[0], list(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDDs and Key Value Pairs\n",
    "\n",
    "Now that we've worked with RDDs and how to aggregate values with them, we can begin to look into working with Key Value Pairs. In order to do this, let's create some fake data as a new text file.\n",
    "\n",
    "This data represents some services sold to customers for some SAAS business.\n",
    "\n",
    "First, let's load our file into Databricks by using Data section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark supports various filesystems (S3, HDFS, etc.), but in our symple case we will load a locally stored file using \n",
    "\n",
    "`rdd = sc.textFile(\"/path/to/file\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services = sc.textFile('/FileStore/tables/services.txt')\n",
    "services.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start transforming our RDD to a list of rows, where each row is represented by another list conisting of individia cell values. For that we apply Python's `split` function that by default transforms comma-separated elements of a string into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services.map(lambda x: x.split()).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove that first hash-tag `#`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services.map(lambda x: x[1:] if x[0]=='#' else x).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a 'recipe' that includes two transformations described above and trigger the transformations by applying `collect` action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services.map(lambda x: x[1:] if x[0]=='#' else x) \\\n",
    "        .map(lambda x: x.split()).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have transformed our file into a format that will allow us to do useful processing of the data. Usually, when working with a tabular data, one will use SparkSQL frameworks that introduces concept of a DataFrame and provides tools for DataFrame operations. However, right now we will rely basic Spark transformartions instead. \n",
    "\n",
    "Let's find out the total sales per state.\n",
    "\n",
    "First, let's store our previons transformation in a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanServ = services.map(lambda x: x[1:] if x[0]=='#' else x) \\\n",
    "                    .map(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply a `map` transformatin that will result in RDD containing `(key, value)` pairs containing information on transactions that is required for counputing sales per state: (State, Transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanServ.map(lambda lst: (lst[3],lst[-1])).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of `('State', 'Amount')` tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanServ.map(lambda lst: (lst[3],lst[-1]))\\\n",
    "         .filter(lambda x: not x[0]=='State')\\\n",
    "         .take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply `reduceByKey` transformation that will return a dataset of (State, Aggregated_Transactions) key-values, where values for each key (State) are aggregated using the given reduce function that will compute the sum of all the transactions for each state.\n",
    "Notice how it assumes that the first item is the key!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanServ.map(lambda lst: (lst[3],lst[-1]))\\\n",
    "         .filter(lambda x: not x[0]=='State')\\\n",
    "         .reduceByKey(lambda amt1,amt2 : amt1+amt2)\\\n",
    "         .take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we forgot that the amounts are still strings! Let's fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanServ.map(lambda lst: (lst[3],lst[-1]))\\\n",
    "         .filter(lambda x: not x[0]=='State')\\\n",
    "         .reduceByKey(lambda amt1,amt2 : float(amt1)+float(amt2))\\\n",
    "         .take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue our analysis by sorting this output:\n",
    "\n",
    "1. Grab state and amounts.\n",
    "1. Get rid of ('State','Amount').\n",
    "1. Add amounts for each state transforming the amounts into strings.\n",
    "1. Sort results by the amount value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanServ.map(lambda lst: (lst[3],lst[-1]))\\\n",
    "         .filter(lambda x: not x[0]=='State')\\\n",
    "         .reduceByKey(lambda amt1,amt2 : float(amt1)+float(amt2))\\\n",
    "         .sortBy(lambda stateAmount: stateAmount[1], ascending=False)\\\n",
    "         .collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "7_1_spark_tutorial",
  "notebookId": 4436838362988445,
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
