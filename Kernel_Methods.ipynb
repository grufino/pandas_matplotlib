{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Methods (Primal vs. Dual View)\n",
    "\n",
    "In this lab we explore how kernel methods can be used on structured data as long as a kernel function can be defined on pairs of objects of data. Specifically, we will use the dynamic time-warping (DTW) kernel to perform learning on sequences. We then proceed to train a kernelized SVM with the DTW kernel on a sequence data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DTW Kernel\n",
    "\n",
    "Given a metric $d: X \\times X \\rightarrow \\mathbb{R}_{\\geq 0}$ on the input space $X$, the family of *DTW Kernels* is given as:\n",
    "\n",
    "$$ k_{\\text{DTW}}(x, x') = e^{- \\lambda d_{\\text{DTW}}(x, x'; d)}, $$\n",
    "\n",
    "for sequences $x, x' \\in X^+ := \\bigcup_{n \\geq 1}{X^n}$ of lengths $|x|$ and $|x'|$. The *DTW distance metric* $d_{\\text{DTW}}$ is then given by $\\gamma(|x|, |x'|)$, where the helper function $\\gamma$ is defined recursively via:\n",
    "\n",
    "$$ \\gamma(i, j) = \\begin{cases} d(x_i, x_j') + \\min\\left(\\gamma(i-1, j-1), \\gamma(i-1, j), \\gamma(i, j-1)\\right) & (1 \\leq i \\leq |x|, \\, 1 \\leq j \\leq |x'|), \\\\ \n",
    "\\infty & i = 0 \\vee j = 0, \\\\\n",
    "0 & (i, j) = (0, 0). \\end{cases}\n",
    "$$\n",
    "\n",
    "The intuition is that $\\gamma(i, j)$ is the minimum squared distance up to time $i$ and $j$. $i = |x|$ and $j = |x'|$ are edge cases in the sense that the if a sequence has ended it cannot be matched anymore (and thus the value are infinite or the result value as both have been matched).\n",
    "\n",
    "To compute $d_{\\text{DTW}}$ the technique of <a href=\"https://en.wikipedia.org/wiki/Dynamic_programming\" target=\"_blank\">Dynamic Programming</a> is being used, where you store $\\gamma$ in a $(|x|+1) \\times (|x'|+1)$ grid.\n",
    "\n",
    "<b>Exercise 1</b>:\n",
    "\n",
    "Implement the function *d_DTW(x, x2, dist)*. The inputs x and x2 are the sequences to be compared and the parameter dist is a function on a pairs of points of the input space $X$ that outputs a real number (the distance between the pairs of points). Some code is given to help you dealing with the edge cases. The function is supposed to return the value of $d_{\\text{DTW}}$ with the specified parameters, *not* the $k_{\\text{DTW}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_DTW(x, x2, dist):\n",
    "    t1, t2 = len(x), len(x2)\n",
    "\n",
    "    if x == [] and x2 == []:\n",
    "        return 0.0\n",
    "    elif (x == []) or (x2 == []):\n",
    "        return np.infty\n",
    "    \n",
    "    dp = np.empty((t1+1, t2+1))\n",
    "\n",
    "    dp[0, 0] = 0\n",
    "\n",
    "    for i in range(1, t1+1):\n",
    "        dp[i, 0] = np.infty\n",
    "    \n",
    "    for j in range(1, t2+1):\n",
    "        dp[0, j] = np.infty\n",
    "\n",
    "    for i in range(1, t1+1):\n",
    "        for j in range(1, t2+1):\n",
    "            dp[i][j] = dist(x[i-1],x2[j-1]) + min(\n",
    "                    dp[i - 1][j - 1], dp[i - 1][j], dp[i][j - 1]\n",
    "                )\n",
    "    return dp[t1, t2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert d_DTW([1, 2, 3, 4], [1, 2, 3], lambda x, y: 1 if x != y else 0) == 1.0\n",
    "    assert d_DTW([1, 2, 3, 3], [1, 2, 3], lambda x, y: 1 if x != y else 0) == 0.0\n",
    "    assert d_DTW([1, 2, 3, 4], [1, 2, 3], lambda x, y: 1 if x != y else 0) == 1.0\n",
    "    assert d_DTW([1, 2, 3, 2], [1, 2], lambda x, y: 1 if x != y else 0) == 1.0\n",
    "    assert d_DTW([], [1, 2], lambda x, y: 1 if x != y else 0) == np.infty\n",
    "    assert d_DTW([], [], lambda x, y: 1 if x != y else 0) == 0.0\n",
    "    print (\"There is no error in your function!\")\n",
    "except AssertionError:\n",
    "    print (\"There is an error in your function!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define three distance functions on two values $x, x' \\in X$:\n",
    "\n",
    "$d_1(x_2, x_2) = \\mathbb{1}[x_1 != x_2]$,\n",
    "\n",
    "$d_2(x_1, x_2) = (x_1 - x_2)^2$,\n",
    "\n",
    "$d_3(x_1, x_2) = |x_1 - x_2|$,\n",
    "\n",
    "Optional: $d_4(\\Delta x_i, \\Delta x'_i) = (\\Delta x_i - \\Delta x'_i)^2$, with\n",
    "$$ \\Delta x_i = \\frac{1}{2}\\left( x_i - x_{i-1} + \\frac{x_{i+1} - x_{i-1}}{2}\\right) $$\n",
    "as *approximate derivates of order 2*. Note that the edge cases are $\\Delta x_1 = 0$ and $\\Delta x_{|x|} = x_{|x|} - x_{|x|-1}$. \n",
    "\n",
    "*Hint*: It's best to map the sequences $x = (x_1, \\dots, x_{|x|})$ to $\\Delta x = \\left(\\Delta x_1, \\dots, \\Delta x_{|x|}\\right)$ and then apply $d_2$.\n",
    "\n",
    "<b>Exercise 2</b>:\n",
    "\n",
    "Implement the missing distance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def d1(x, x2):\n",
    "    return 1 if x != x2 else 0\n",
    "\n",
    "def d2(x, x2):\n",
    "    return np.square(x - x2)\n",
    "\n",
    "def d3(x, x2):\n",
    "    return np.abs(np.subtract(x, x2))\n",
    "\n",
    "# def d4(x, x2):\n",
    "#     delta_x = []\n",
    "#     for i in range(1, len(x)):\n",
    "#         delta_x.append(1/2*(x[i] - x[i-1] + (x[i+1] - x[i-1]/2)))\n",
    "#\n",
    "#     delta_x2 = []\n",
    "#     for i in range(1, len(x2)):\n",
    "#         delta_x2.append(1/2*(x2[i] - x[i-1] + (x2[i+1] - x2[i-1]/2)))\n",
    "#\n",
    "#     return d2(delta_x, delta_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code lifts the distance metrics to maps that map a given hyperparameter $\\lambda$ return the corresponding kernel function $k_{\\text{DTW}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k1_hyp, k2_hyp, k3_hyp = [lambda lmbd: (lambda x, x2: np.exp(-lmbd * d_DTW(x, x2, d))) for d in [d1, d2, d3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k1 = k1_hyp(2.0)\n",
    "k2 = k2_hyp(2.0)\n",
    "k3 = k3_hyp(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code computes the Gram matrix $K$ with respect to the kernel $k$ (a parameter) and the data $xs$ (another parameter), see slide 28 and 29 in Kernel Methods lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dtw_gram_matrix(xs, x2s, k):\n",
    "    \"\"\"\n",
    "    xs: collection of sequences (vectors of possibly varying length)\n",
    "    x2s: the same, needed for prediction\n",
    "    k: a kernel function that maps two sequences of possibly different length to a real\n",
    "    The function returns the Gram matrix with respect to k of the data xs.\n",
    "    \"\"\"\n",
    "    t1, t2 = len(xs), len(x2s)\n",
    "    K = np.empty((t1, t2))\n",
    "    \n",
    "    for i in range(t1):\n",
    "        for j in range(i, t2):\n",
    "            K[i, j] = k(xs[i], x2s[j])\n",
    "            if i < t2 and j < t1:\n",
    "                K[j, i] = K[i, j]\n",
    "        \n",
    "    return K\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "build_dtw_gram_matrix([[1, 2], [2, 3]], [[1, 2, 3], [4]], k1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kernel SVM\n",
    "\n",
    "Now we implement the training algorithm for kernel SVMs. We adjust the ERM learning algorithm from the linear classification lab. First we are reusing the code for the $\\mathcal{L}_2$-regularizer and the hinge loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L2_reg(w, lbda):\n",
    "    return 0.5 * lbda * (np.dot(w.T, w)), lbda*w\n",
    "\n",
    "def hinge_loss(h, y):\n",
    "    n = len(h)\n",
    "    l = np.maximum(0, np.ones(n) - y*h)\n",
    "    g = -y * (h > 0)\n",
    "    return l, g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 3</b>:\n",
    "\n",
    "Adjust the old code (Lab 06) to actually learn the kernel linear regression. Note that there is a new parameter $k$ that encodes the kernel function. Note that lbda is not the $\\lambda$ used in the definition of $k$, but the regularization coefficient (as before). Note also that the learning rate $\\alpha$ has been renamed to $\\eta$, because $\\alpha$ coincides with the dual coefficients (see lecture).\n",
    "Also make sure to return the Gram matrix $K$ together with the weight vector $w$ (or $\\alpha$), as it is costly to compute and needed for the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_reg_kernel_ERM(X, y, lbda, k, loss=hinge_loss, reg=L2_reg, max_iter=200, tol=0.001, eta=1., verbose=False):\n",
    "    \"\"\"Kernel Linear Regression (default: kernelized L_2 SVM)\n",
    "    X -- data, each row = instance\n",
    "    y -- vector of labels, n_rows(X) == y.shape[0]\n",
    "    lbda -- regularization coefficient lambda\n",
    "    k -- the kernel function\n",
    "    loss -- loss function, returns vector of losses (for each instance) AND the gradient\n",
    "    reg -- regularization function, returns reg-loss and gradient\n",
    "    max_iter -- max. number of iterations of gradient descent\n",
    "    tol -- stop if norm(gradient) < tol\n",
    "    eta -- learning rate\n",
    "    \"\"\"\n",
    "    num_features = X.shape[1]\n",
    "\n",
    "    g_old = None\n",
    "\n",
    "    K = build_dtw_gram_matrix(X, X, k1) # MODIFY; fill in; hint: use gram matrix defined above\n",
    "    w = np.random.randn(K.shape[0]) # modify; hint: w has as many entries as training examples (K.shape[0])\n",
    "    h = np.dot(w,K)# MODIFY; hint: see slide 20,21, and 35 (primal vs. dual view)\n",
    "    l,lg = loss(h, y)\n",
    "    for _ in range(max_iter):\n",
    "        if verbose:\n",
    "            print('training loss: ' + str(np.mean(l)))\n",
    "            \n",
    "        r,rg = reg(w, lbda)\n",
    "        g = lg + rg \n",
    "        \n",
    "        if g_old is not None:\n",
    "            eta = eta*(np.dot(g_old.T,g_old))/(np.dot((g_old - g).T, g_old)) # MODIFY\n",
    "            # hint: gram matrix K changes scalar product from <x, x'> = x^T x to x^T K x\n",
    "            \n",
    "        w = w - eta*g\n",
    "        if (np.linalg.norm(eta*g)<tol):\n",
    "            break\n",
    "        g_old = g\n",
    "        \n",
    "    return w, K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjusted inference function is given as (for binary classification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(alpha, X, X_train, k):\n",
    "    K = build_dtw_gram_matrix(X_train, X, k)\n",
    "    y_pred = np.dot(K, alpha)\n",
    "    y_pred[y_pred >= 0] = 1\n",
    "    y_pred[y_pred < 0] = -1\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DTW Kernel SVM in Action\n",
    "\n",
    "Now we put our results from section $1$ and $2$ together to use a kernelized SVM for a classification task on sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import loadmat # for matlab *.mat format, for modern once need to install hdf5\n",
    "\n",
    "file_path = \"data/laser_small.mat\" # file path for multi os support\n",
    "mat = loadmat(file_path)\n",
    "\n",
    "X = mat['X']\n",
    "y = mat['Y'].reshape(50)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have only 50 training instances and thus only go for a simple train-test-split (we cannot afford a simple train-val-test-split). If we try several kernels, we are actually tuning a hyperparameter and thus are fitting on the test set. The solution to this problem would be the nested cross-validation procedure, which we learn in the evaluation lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha, K = learn_reg_kernel_ERM(X_train, y_train, lbda=1, k=k2, max_iter=20000, eta=1, tol=1e-3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = predict(alpha, X_train, X_train, k2)\n",
    "print (\"Training Accuracy: {}\".format(np.mean(y_train == y_pred)))\n",
    "print (\"Test Accuracy: {}\".format(np.mean(y_test == predict(alpha,X_train, X_test, k2))))\n",
    "print (\"Shape of alpha {}\".format(alpha.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the training accuracy is far better than the test accuracy. This *could* - but does not have to - mean that we are overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vary the choices of the kernel functions, regularization parameters and kernel smoothing parameters (the $\\lambda$ in the definition of $k_{\\text{DTW}}$). In the rest of the notebook you learn how you can draw learning curves we have discussed in the tutorial. To be able to use the helper function, the estimator needs to be wrapped in a scikit-learn conform way. You can find and use the example class KernelEstimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.learning_curve import learning_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1,\n",
    "                        train_sizes=10, # list of floats that describe ratio of test data sets tried\n",
    "                        # OR an int = # how many trials\n",
    "                        scoring=None):\n",
    "\n",
    "    if type(train_sizes) == int:\n",
    "        train_sizes=np.linspace(.1, 1.0, train_sizes)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    " \n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    if cv is not None:\n",
    "        plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    \n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    if cv is not None:\n",
    "        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class KernelEstimator(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, k, lbda):\n",
    "        self.k = k\n",
    "        self.lbda = lbda\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self._X_train = X\n",
    "        self._alpha, _ = learn_reg_kernel_ERM(X, y, lbda=self.lbda, k=self.k, max_iter=20000, eta=1, tol=1e-3)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return predict(self._alpha, self._X_train, X, self.k)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y == y_pred)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercise 4:</b>\n",
    "\n",
    "Vary the choices of the kernel functions, regularization parameters and kernel smoothing parameters (the $\\lambda$ in the definition of $k_{\\text{DTW}}$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-599-637e17b7ef2a>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mestimator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m\"Accuracy {}\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mestimator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscore\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mplot_learning_curve\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mKernelEstimator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mk2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2.0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'Euclidean distance DTW, lambda = 1.0'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcv\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscoring\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"accuracy\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_sizes\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0.5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0.6\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0.7\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0.8\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0.9\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1.0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-597-33d39bbac398>\u001B[0m in \u001B[0;36mplot_learning_curve\u001B[0;34m(estimator, title, X, y, ylim, cv, n_jobs, train_sizes, scoring)\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mxlabel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Training examples\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m     \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mylabel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Score\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 18\u001B[0;31m     train_sizes, train_scores, test_scores = learning_curve(\n\u001B[0m\u001B[1;32m     19\u001B[0m         estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n\u001B[1;32m     20\u001B[0m     \u001B[0mtrain_scores_mean\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_scores\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pandas_matplotlib/venv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m             \u001B[0;31m# extra_args > 0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pandas_matplotlib/venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\u001B[0m in \u001B[0;36mlearning_curve\u001B[0;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, shuffle, random_state, error_score, return_times, fit_params)\u001B[0m\n\u001B[1;32m   1410\u001B[0m                 \u001B[0mtrain_test_proportions\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mn_train_samples\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1411\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1412\u001B[0;31m         results = parallel(delayed(_fit_and_score)(\n\u001B[0m\u001B[1;32m   1413\u001B[0m             \u001B[0mclone\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mestimator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscorer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1414\u001B[0m             \u001B[0mparameters\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfit_params\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreturn_train_score\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pandas_matplotlib/venv/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1042\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_iterating\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_original_iterator\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1043\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1044\u001B[0;31m             \u001B[0;32mwhile\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdispatch_one_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1045\u001B[0m                 \u001B[0;32mpass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1046\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pandas_matplotlib/venv/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36mdispatch_one_batch\u001B[0;34m(self, iterator)\u001B[0m\n\u001B[1;32m    857\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    858\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 859\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dispatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtasks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    860\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    861\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pandas_matplotlib/venv/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36m_dispatch\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    775\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    776\u001B[0m             \u001B[0mjob_idx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jobs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 777\u001B[0;31m             \u001B[0mjob\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply_async\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallback\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    778\u001B[0m             \u001B[0;31m# A job can complete so quickly than its callback is\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    779\u001B[0m             \u001B[0;31m# called before we get here, causing self._jobs to\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pandas_matplotlib/venv/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001B[0m in \u001B[0;36mapply_async\u001B[0;34m(self, func, callback)\u001B[0m\n\u001B[1;32m    206\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mapply_async\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallback\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    207\u001B[0m         \u001B[0;34m\"\"\"Schedule a func to be run\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 208\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mImmediateResult\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    209\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcallback\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    210\u001B[0m             \u001B[0mcallback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pandas_matplotlib/venv/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    570\u001B[0m         \u001B[0;31m# Don't delay the application, to avoid keeping the input\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    571\u001B[0m         \u001B[0;31m# arguments in memory\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 572\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    573\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    574\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pandas_matplotlib/venv/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    260\u001B[0m         \u001B[0;31m# change the default number of processes to -1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    261\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mparallel_backend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_jobs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_n_jobs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 262\u001B[0;31m             return [func(*args, **kwargs)\n\u001B[0m\u001B[1;32m    263\u001B[0m                     for func, args, kwargs in self.items]\n\u001B[1;32m    264\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pandas_matplotlib/venv/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    260\u001B[0m         \u001B[0;31m# change the default number of processes to -1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    261\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mparallel_backend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_jobs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_n_jobs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 262\u001B[0;31m             return [func(*args, **kwargs)\n\u001B[0m\u001B[1;32m    263\u001B[0m                     for func, args, kwargs in self.items]\n\u001B[1;32m    264\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/pandas_matplotlib/venv/lib/python3.9/site-packages/sklearn/utils/fixes.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    220\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    221\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mconfig_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 222\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunction\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/PycharmProjects/pandas_matplotlib/venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\u001B[0m in \u001B[0;36m_fit_and_score\u001B[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[0m\n\u001B[1;32m    596\u001B[0m             \u001B[0mestimator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    597\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 598\u001B[0;31m             \u001B[0mestimator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    599\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    600\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-598-f7404976da04>\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_X_train\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_alpha\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlearn_reg_kernel_ERM\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlbda\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlbda\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_iter\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m20000\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0meta\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1e-3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-591-337b6880f9c0>\u001B[0m in \u001B[0;36mlearn_reg_kernel_ERM\u001B[0;34m(X, y, lbda, k, loss, reg, max_iter, tol, eta, verbose)\u001B[0m\n\u001B[1;32m     15\u001B[0m     \u001B[0mg_old\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m     \u001B[0mK\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuild_dtw_gram_matrix\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mk1\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# MODIFY; fill in; hint: use gram matrix defined above\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m     \u001B[0mw\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrandn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mK\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# modify; hint: w has as many entries as training examples (K.shape[0])\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0mh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mw\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mK\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m# MODIFY; hint: see slide 20,21, and 35 (primal vs. dual view)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-588-2ddff689aad8>\u001B[0m in \u001B[0;36mbuild_dtw_gram_matrix\u001B[0;34m(xs, x2s, k)\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mt1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mt2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m             \u001B[0mK\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mj\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mxs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx2s\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     14\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mi\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0mt2\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mj\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0mt1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m                 \u001B[0mK\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mK\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mj\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-586-fbf398a103c4>\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(x, x2)\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mk1_hyp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mk2_hyp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mk3_hyp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mlmbd\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx2\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexp\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mlmbd\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0md_DTW\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0md\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0md\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0md1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0md2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0md3\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-583-7def6467d7a9>\u001B[0m in \u001B[0;36md_DTW\u001B[0;34m(x, x2, dist)\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mt1\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mt2\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 21\u001B[0;31m             dp[i][j] = dist(x[i-1],x2[j-1]) + min(\n\u001B[0m\u001B[1;32m     22\u001B[0m                     \u001B[0mdp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m                 )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "estimator = KernelEstimator(k1, 2.0)   # MODIFY\n",
    "estimator.fit(X_train, y_train)\n",
    "print (\"Accuracy {}\".format(estimator.score(X_train, y_train)))\n",
    "plot_learning_curve(KernelEstimator(k2, 2.0), 'Euclidean distance DTW, lambda = 1.0', X_train, y_train, cv=None, scoring=\"accuracy\", train_sizes=[0.5, 0.6, 0.7, 0.8, 0.9, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}